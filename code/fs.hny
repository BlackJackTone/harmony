from synch import *             # shared queue for file server and lock for superblock
from alloc import *             # malloc/free
from RW import *                # read/write locks for inode blocks
from list import subseq         # list slicing

const N_BLOCKS = 10
const INODES_PER_BLOCK = 2      # number of inodes that fit in a block
const INDIR_PER_BLOCK = 4       # number of block pointers per block

#### BLOCK SERVER INTERFACE ####
#
# Both the disk and file servers are block servers.  In the interfaces below,
# req_q is the queue for requests, while res_q is the queue for responses.

def bs_getsize(req_q, res_q, ino) returns size:
    put(req_q, { .type: "getsize", .ino: ino, .q: res_q })
    size = get(res_q)

def bs_read(req_q, res_q, ino, offset) returns block:
    put(req_q, { .type: "read", .ino: ino, .offset: offset, .q: res_q })
    block = get(res_q)

def bs_write(req_q, res_q, ino, offset, data):
    put(req_q, { .type: "write", .ino: ino, .offset: offset, .data: data, .q: res_q })
    let status = get(res_q):
        assert status == "ok"

#### DISK CODE ####

disk = [None,] * N_BLOCKS            # Shared disk

def disk_getsize() returns size:
    size = N_BLOCKS

def disk_read(bno) returns block:
    block = disk[bno]

def disk_write(bno, block):
    disk[bno] = block

#### FILE SERVER CODE ####
#
# The file system consists of a superblock, an array of inode blocks, and
# the remaining blocks.  The remaining blocks are dynamic and can be of
# the following types:
#   - free: not in use
#   - data: a data block
#   - indir: an indirect block, with pointers to other blocks
# An inode has a pointer to a direct block and a pointer to an indirect block,
# so the maximum file size is 1 + INDIR_PER_BLOCK.

# Put block bno on the free list.  The free list is a linked list of
# indirect blocks.  The first entry in a free list block points to the
# next free list block.  The other entries point to free blocks.  The
# superblock points to the first free list block.
def fs_release(res_q, fs_state, bno):
    acquire(?fs_state->super_lock)
    var super = disk_read(0)
    if super.free == None:
        disk_write(bno, [ None, ])
        super.free = bno
        disk_write(0, super)
    else:
        let fb = disk_read(super.free):
            if len(fb) == INDIR_PER_BLOCK:
                # The first free list block is full
                disk_write(bno, [ super.free, ])
                super.free = bno
                disk_write(0, super)
            else:
                disk_write(super.free, fb + [ bno, ])
    release(?fs_state->super_lock)

# Allocate a disk block
def fs_alloc(res_q, fs_state) returns bno:
    acquire(?fs_state->super_lock)
    var super = disk_read(0)
    if super.free == None:
        bno = None
    else:
        let fb = disk_read(super.free):
            if len(fb) == 1:
                bno = super.free
                super.free = fb[0]
                disk_write(0, super)
            else:
                bno = fb[len(fb) - 1]
                disk_write(super.free, subseq(fb, 0, len(fb) - 1))
    release(?fs_state->super_lock)

# Initialize the file system by writing the superblock, the i-node blocks,
# and creating the free list.
def fs_init(fs_state, n_inode_blocks):
    let res_q = malloc(Queue()):
        # Initialize the superblock
        disk_write(0, { .n_inode_blocks: n_inode_blocks, .free: None })

        # Initialize the i-node blocks
        for i in { 1 .. n_inode_blocks }:
            disk_write(i, [
                { .direct: None, .indir: None, .size: 0 }, ] * INODES_PER_BLOCK)

        # Initialize the free list
        let n_disk_blocks = disk_getsize():
            for i in { n_inode_blocks + 1 .. n_disk_blocks - 1 }:
                fs_release(res_q, fs_state, i)

        free(res_q)

# Handle a request for the file server.  res_q is the response queue to the
# client, disk is the request queue to the disk server, and req is the request
def fs_handle_request(res_q, fs_state, req):
    let ib = req.ino / INODES_PER_BLOCK:
        # Grab a lock on the inode block
        if req.type == "write":
            write_acquire(?fs_state->ib_locks[ib])
        else:
            read_acquire(?fs_state->ib_locks[ib])

        # Read the inode block and extract the inode
        var inode_block = disk_read(1 + ib)
        var inode = inode_block[req.ino % INODES_PER_BLOCK]

        if req.type == "getsize":
            put(req.q, inode.size)

        elif req.type == "read":
            # Read the direct block.  Return None if there is no direct block.
            if req.offset == 0:
                if inode.direct == None:
                    put(req.q, disk_read(None))
                else:
                    put(req.q, disk_read(inode.direct))

            # Read indirectly.  If there is no indirect block return None
            elif inode.indir == None:
                put(req.q, disk_read(None))

            # Read the indirect block and get the pointer to the data block,
            # which may be None.
            else:
                let indir = disk_read(inode.indir):
                    if indir[req.offset - 1] == None:
                        put(req.q, disk_read(None))
                    else:
                        put(req.q, disk_read(indir[req.offset - 1]))

        else:
            assert req.type == "write"
            # Write the direct block.  Allocate one if needed, and if so update
            # the inode.  If not, just update the data block.
            if req.offset == 0:
                if inode.direct == None:
                    inode.direct = fs_alloc(res_q, fs_state)
                    assert inode.direct != None
                    inode.size = max(inode.size, 1)
                    inode_block[req.ino % INODES_PER_BLOCK] = inode
                    disk_write(1 + ib, inode_block)
                disk_write(inode.direct, req.data)

            # Write a block indirectly
            else:
                # Allocate an indirect block first if there isn't one.  Note that
                # the inode block, indirect block, and data block must all be written
                if inode.indir == None:
                    inode.indir = fs_alloc(res_q, fs_state)
                    assert inode.indir != None
                    inode.size = max(inode.size, req.offset + 1)
                    inode_block[req.ino % INODES_PER_BLOCK] = inode
                    disk_write(1 + ib, inode_block)
                    let bno = fs_alloc(res_q, fs_state)
                    let indir = [ bno if i == (req.offset - 1) else None
                                    for i in { 0 .. INODES_PER_BLOCK - 1 } ]:
                        assert bno != None
                        disk_write(bno, req.data)
                        disk_write(inode.indir, indir)

                # Read the indirect block first.  If needed allocate a data block,
                # otherwise just overwrite the existing data block.
                else:
                    var indir = disk_read(inode.indir)
                    if indir[req.offset - 1] == None:
                        indir[req.offset - 1] = fs_alloc(res_q, fs_state)
                        assert indir[req.offset - 1] != None
                        disk_write(inode.indir, indir)
                    disk_write(indir[req.offset - 1], req.data)
                    if inode.size <= req.offset:
                        inode.size = req.offset + 1
                        inode_block[req.ino % INODES_PER_BLOCK] = inode
                        disk_write(1 + ib, inode_block)

                # Response from write request is ok
                put(req.q, "ok")

            # Release the lock on the inode block
            if req.type == "write":
                write_release(?fs_state->ib_locks[ib])
            else:
                read_release(?fs_state->ib_locks[ib])

# A worker thread handles client requests
def fs_worker(fs_state):
    let res_q = malloc(Queue()):
        while True:
            fs_handle_request(res_q, fs_state, get(?file_queue))

# The file server.  Initialize the file system and spawn worker threads
def file_server(n_inode_blocks, n_workers):
    # The in-memory shared state of the file server consists of:
    #   super_lock: lock on the superblock (and free list in particular)
    #   ib_locks:   read/write locks on inode blocks
    let fs_state = malloc({
            .super_lock: Lock(),
            .ib_locks: [ RWlock(), ] * n_inode_blocks }):

        # Initialize the file system on disk
        fs_init(fs_state, n_inode_blocks)

        # Start worker threads to handle client requests
        for i in { 1 .. n_workers }:
            spawn eternal fs_worker(fs_state)

#### TESTING CODE ####

def client(ino, offset, string):
    let res_q = malloc(Queue()):
        bs_write(?file_queue, res_q, ino, offset, string)
        let data = bs_read(?file_queue, res_q, ino, offset):
            assert data == string, (data, string)
        let size = bs_getsize(?file_queue, res_q, ino):
            assert size == (offset + 1), (size, offset)

# Shared queue for clients to send requests to the workers
file_queue = Queue()

spawn file_server(2, 2)
spawn client(1, 1, "hello")
spawn client(2, 1, "world")
